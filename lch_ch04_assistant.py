# ê¸°ë³¸ ì •ë³´ ì…ë ¥

import streamlit as st

# from audiorecorder import audiorecorder
# from streamlit_audiorecorder import audiorecorder
from audio_recorder_streamlit import audio_recorder

from openai import OpenAI

import os
import base64
import numpy as np

# ê¸°ëŠ¥ êµ¬í˜„ í•¨ìˆ˜
def STT(audio, client):
    # Whisper APIê°€ íŒŒì¼ í˜•íƒœë¡œ ìŒì„±ì„ ì…ë ¥ë°›ìœ¼ë¯€ë¡œ input.mp3ë¡œ ì €ì¥
    filename = "input.mp3"
    # wav_file = open(filename, "wb")
    # wav_file.write(audio.export().read())
    # wav_file.close()

    with open(filename, "wb") as f:
        f.write(audio)

    # ìŒì„± íŒŒì¼ ì—´ê¸°
    audio_file = open(filename, "rb")
    # Whisper ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì–»ê¸°
    try: 
        # openaiì˜ whisper APIë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        transcript = client.audio.transcriptions.create(
            model="whisper-1", 
            file=audio_file, 
            response_format="text"
        )

        audio_file.close()
        os.remove(filename)  # íŒŒì¼ ì‚­ì œ
    except Exception as e:
        transcript = f"ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}"
    return transcript

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” TTS(Text-to-Speech) API
def TTS(reponse, client):
    with client.audio.speech.with_streaming_response.create(
        model="tts-1",
        voice="alloy",
        input=reponse,
    ) as response:
        # ìŒì„± íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.
        filename = "output.mp3"
        response.stream_to_file(filename)

    # ì €ì¥í•œ ìŒì„± íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì¬ìƒ
    with open(filename, "rb") as f: 
        data = f.read()
        b64 = base64.b64encode(data).decode()

        # ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ìŒì„± ìë™ ì¬ìƒ
        md = f"""
            <audio autoplay="True">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(md, unsafe_allow_html=True,)   # streamlit ì•ˆì—ì„œ HTML ë¬¸ë²• êµ¬í˜„ì— ì‚¬ìš©ë˜ëŠ” st.markdown()ì„ í™œìš©í•˜ì—¬ ì‹¤í–‰

    # í´ë”ì— ë‚¨ì§€ ì•Šë„ë¡ íŒŒì¼ ì‚­ì œ
    os.remove(filename)  # íŒŒì¼ ì‚­ì œ

# ìŒì„± ë¹„ì„œì˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ChatGPT API
def ask_gpt(prompt, client):
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=prompt
    )
    return response.choices[0].message.content
    
### ë©”ì¸ í•¨ìˆ˜ ###
def main():
    # API í‚¤ ì…ë ¥
    client = OpenAI(api_key="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")
                             
    # í™”ë©´ ìƒë‹¨ì— í‘œì‹œë  í”„ë¡œê·¸ë¨ ì´ë¦„
    st.set_page_config(page_title="LCH ìŒì„± ë¹„ì„œ", layout="wide")

    
    if "check_audio" not in st.session_state:
        st.session_state["check_audio"] = None # í”„ë¡œê·¸ë¨ì´ ì¬ì‹¤í–‰ë ë•Œë§ˆë‹¤ ì´ì „ ë…¹ìŒ íŒŒì¼ ì •ë³´ê°€ ë²„í¼ì— ë‚¨ì•„ìˆì–´ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì´ì „ ë…¹ìŒ íŒŒì¼ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "system", "content": "You are a thoughful assistant. Respond to all input in 25 words and answer in Korean"}] 
        # GPT APIì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ì–‘ì‹. ì´ì „ ì§ˆë¬¸ ë° ë‹µë³€ì„ ëˆ„ì í•˜ì—¬ ì €ì¥.

    # ê¸°ëŠ¥ êµ¬í˜„ ê³µê°„
    col1, col2 = st.columns(2) # í™”ë©´ì„ ë‘ ê°œì˜ ì—´ë¡œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•©ë‹ˆë‹¤.

    with col1: # ìŒì„± ë¹„ì„œ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤. (ì™¼ìª½ ê³µê°„)
        st.header("AI Assistant ğŸ”Š")
        st.image('ai.png', width=200) # AI ë¹„ì„œ ì´ë¯¸ì§€ ì‚½ì…
        st.markdown('---')

        flag_start = False # ìŒì„± ë¹„ì„œ ì‹œì‘ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í”Œë˜ê·¸ ë³€ìˆ˜ì…ë‹ˆë‹¤.

        audio = audio_recorder("ì§ˆë¬¸", "ë…¹ìŒ ì¤‘ ...")

        if audio is not None and len(audio) > 0 and st.session_state["check_audio"] is not audio:
            #  st.audio(audio.export().read()) # ë…¹ìŒëœ ìŒì„±ì„ ì¬ìƒí•©ë‹ˆë‹¤.
            st.audio(audio) # ë…¹ìŒëœ ìŒì„±ì„ ì¬ìƒí•©ë‹ˆë‹¤.
            question = STT(audio, client) # STT í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

            st.session_state["messages"] = st.session_state["messages"] + [{"role": "user", "content": question}] # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë©”ì‹œì§€ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            st.session_state["check_audio"] = audio # ë…¹ìŒëœ ìŒì„±ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.
            flag_start = True # ìŒì„± ë¹„ì„œ ì‹œì‘ í”Œë˜ê·¸ë¥¼ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

    with col2:
        st.header('ëŒ€í™”ê¸°ë¡ ğŸ”Š')
        if flag_start: # ìŒì„± ë¹„ì„œê°€ ì‹œì‘ë˜ë©´ ëŒ€í™” ê¸°ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤.         
            response = ask_gpt(st.session_state["messages"], client) # ask_gpt í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ChatGPTì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. 
            st.session_state["messages"] = st.session_state["messages"] + [{"role": "assistant", "content": response}] # ChatGPTì˜ ë‹µë³€ì„ ë©”ì‹œì§€ì— ì¶”ê°€í•©ë‹ˆë‹¤.

            for message in st.session_state["messages"]: # # ëŒ€í™” ê¸°ë¡ì„ í‘œì‹œí•©ë‹ˆë‹¤.                    
                if message["role"] != "system": # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
                    with st.chat_message(message["role"]): ## ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ChatGPTì˜ ë‹µë³€ì„ í‘œì‹œí•©ë‹ˆë‹¤.
                        st.markdown(message["content"]) ## ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ChatGPTì˜ ë‹µë³€ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
            
            TTS(response, client)

if __name__ == "__main__":
    main()


        